# -*- coding: utf-8 -*-

"""Apple Stock Price Prediction.ipynb
- Sentiment Analysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D7GgqXwLmkuswt8TFRp_fsU_pgwhoNSW

# Apple Stock Price Prediction Based on News and Historical Stock Price Data

# 1. Import Libraries
"""

# general usage
import numpy as np
import pandas as pd
from typing import Tuple

# for data preprocessing
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
nltk.download('punkt')
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import CountVectorizer

# for data modelling
from xgboost import XGBClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# for evaluation
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

"""# 2. Data Understanding

## Apple Stock Data

• 2301 rows and 7 columns

• Dates:
( 08 June, 2008 to 30 June, 2016)

• Contains some missing days
(e.g. days on which the stock market doesn't open)

Apple stock data downloaded from Yahoo Finance - from June 8, 2008 to June 30, 2016
"""

stock_prices = pd.read_csv("AAPL.csv")
stock_prices.head(2)

"""See if there is any null data in the dataset."""

stock_prices.isnull().sum()

"""## News Data

Date: Time of the news 
08 June, 2008 to 01 January, 2016

News: News headline 

73607 rows and 2 columns 
(Some news are from the same day)

News data downloaded from Canvas
"""

news = pd.read_csv('RedditNews.csv')
news.head(2)

"""See if there is any null data in the dataset."""

news.isnull().sum()

"""# 3. Data Preprocessing

## News Data: Merge the news that are posted in the same day into one row.
"""

aggregation_functions = {'News': ' '.join}
news = news.groupby(news['Date']).aggregate(aggregation_functions)

print(news.iloc[0,0])

"""## Inner join the stock price dataset and the news dataset.

## Drop data that we are not using.
"""

# join the datasets
base_data = pd.merge(stock_prices, news, on='Date', how='inner')

# delete unnecessary data
base_data.drop(columns=['Adj Close', 'Date'], inplace=True)
del news
del stock_prices

base_data.head(2)

"""## Build a Label column.
* 0 if the stock price drops
* 1 otherwise
"""

base_data['Label'] = np.where(base_data['Close'].shift(-1) - base_data['Close'] < 0, 0, 1)
base_data.head(2)

"""## News Data: Remove punctuations and stopwords, and convert to lowercase."""

# create a set to store the punctuations and stopwords
tokens_to_be_removed = set(stopwords.words())
tokens_to_be_removed.update(string.punctuation)

# remove punctuations and stopwords froma news, and convert the news to lowercase
def process_news(news: str) -> str:
    # tokenize the words
    tokens = word_tokenize(news)

    # remove punctuations and stopwords
    tokens = [token for token in tokens if token not in tokens_to_be_removed]

    # join the tokens and convert to lowercase
    return ' '.join(tokens).lower()

# apply process_news to the news
base_data['News'] = base_data['News'].apply(process_news)
base_data['News'].head(2)

"""## Use CountVectorizer to vectorize the news data.
We create a function that returns the n-gram vectorized version of dataset.
"""

def data_ngram(n: int) -> pd.DataFrame:
    '''
    Function that returns the dataset.
    The parameter n determines the n-gram range.
    '''

    # vectorize the news data
    countvectorizer = CountVectorizer(ngram_range=(1,n), max_features=100000)
    vectors = countvectorizer.fit_transform(base_data['News']).toarray()
    vectors = pd.DataFrame(vectors, columns=[str(i) for i in range(vectors.shape[1])])

    # return the dataframe that contains the neccessary data
    return pd.concat((base_data[['Open', 'High', 'Low', 'Close', 'Label']], vectors), axis=1, join='inner', copy=False)

"""## Separate data into training datasets and testing datasets that are standardized by StandardScaler.
We create a function to return the training and testing datasets.

The function should be used like this if you want to get the 4-gram version of dataset:
```
X_train, X_test, y_train, y_test = train_test_datasets(4)
```


"""

def train_test_datasets(n: int) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    # get the data
    data = data_ngram(n)

    # split the dataset into train and test datasets with ratio 8 : 2
    train_num = int(0.8 * len(data))
    train = data[:train_num]
    test = data[train_num:]

    # standardize the features
    scaler = StandardScaler()
    X_train = scaler.fit_transform(train.drop('Label', axis=1))
    X_test = scaler.transform(test.drop('Label', axis=1))

    # get the label
    y_train = train['Label'].to_numpy()
    y_test = test['Label'].to_numpy()

    return X_train, X_test, y_train, y_test

"""# 4. Modelling

We are using logistic regression model together with the 1-gram,2-gram,3-gram and 4-gram model respectively.

Modelling(1-gram)

Logistic Regression
"""

# 1-gram model training
X_train, X_test, y_train, y_test = train_test_datasets(1)

# model 1: logistic regression
log_md = LogisticRegression()
log_md = log_md.fit(X_train, y_train)

# 2-gram model prediction
predictions = log_md.predict(X_test)

print(classification_report(y_test, predictions))

"""XGBoost"""

# XGBoost Model
org_gbm = XGBClassifier()
org_gbm.fit(X_train,y_train)
y_pred = org_gbm.predict(X_test)

print(classification_report(y_test,y_pred))

"""Fine-tuned logistic regression"""

log=LogisticRegression()
logis_reg_cv = GridSearchCV(log,{'penalty': ['l2','13'], 'C': [0.0001,0.001]})
logis_reg_cv.fit(X_train,y_train)

logis_reg_cv.best_params_

logis_reg_para = LogisticRegression(**logis_reg_cv.best_params_) 
logis_reg_para.fit(X_train, y_train)

logis_reg_predictions_para = logis_reg_para.predict(X_test)
print(classification_report(logis_reg_predictions_para, y_test))

"""Model(2-grams)

Logistic Regression
"""

# 2-gram model training
X_train, X_test, y_train, y_test = train_test_datasets(2)

# model 1: logistic regression
log_md = LogisticRegression()
log_md = log_md.fit(X_train, y_train)

# 2-gram model prediction
predictions = log_md.predict(X_test)

print(classification_report(y_test, predictions))

"""XGBoost"""

# model 2: XGBoost Model
org_gbm = XGBClassifier()
org_gbm.fit(X_train,y_train)
y_pred = org_gbm.predict(X_test)

print(classification_report(y_test,y_pred))

"""Fine-tuned logistic regression"""

log=LogisticRegression()
logis_reg_cv = GridSearchCV(log,{'penalty': ['l2','13'], 'C': [0.1,1,10]})
logis_reg_cv.fit(X_train,y_train)

logis_reg_cv.best_params_

logis_reg_para = LogisticRegression(**logis_reg_cv.best_params_) 
logis_reg_para.fit(X_train, y_train)

logis_reg_predictions_para = logis_reg_para.predict(X_test)
print(classification_report(logis_reg_predictions_para, y_test))

"""Model(3-grams)

Logistic regression
"""

# 3-gram model training
X_train, X_test, y_train, y_test = train_test_datasets(3)

# model 1: logistic regression
log_md = LogisticRegression()
log_md = log_md.fit(X_train, y_train)

# 2-gram model prediction
predictions = log_md.predict(X_test)

print(classification_report(y_test, predictions))

"""XGBoost"""

# XGBoost Model
org_gbm = XGBClassifier()
org_gbm.fit(X_train,y_train)
y_pred = org_gbm.predict(X_test)

print(classification_report(y_test,y_pred))

"""Model(4-gram)

Logistic regression
"""

# 4-gram model training
X_train, X_test, y_train, y_test = train_test_datasets(4)

# model 1: logistic regression
log_md = LogisticRegression()
log_md = log_md.fit(X_train, y_train)

# 2-gram model prediction
predictions = log_md.predict(X_test)

print(classification_report(y_test, predictions))

"""# Findings
* XGBoost is more accurate than logistic regression in our case.
* 3-gram with XGBoost is better than other models in our case.
"""
